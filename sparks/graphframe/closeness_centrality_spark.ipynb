{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphframes import *\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = spark.read.csv(\"data/social-nodes.csv\", header=True)\n",
    "e = spark.read.csv(\"data/social-relationships.csv\", header=True)\n",
    "g = GraphFrame(v, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------+---------+\n",
      "|     id|degree|inDegree|outDegree|\n",
      "+-------+------+--------+---------+\n",
      "|   Doug|     6|       5|        1|\n",
      "|  Alice|     7|       3|        4|\n",
      "|Michael|     5|       2|        3|\n",
      "|Bridget|     5|       2|        3|\n",
      "|Charles|     2|       1|        1|\n",
      "|    Amy|     1|       1|        0|\n",
      "|   Mark|     3|       1|        2|\n",
      "|  David|     2|       1|        1|\n",
      "|  James|     1|       0|        1|\n",
      "+-------+------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_degree = g.degrees\n",
    "in_degree = g.inDegrees\n",
    "out_degree = g.outDegrees\n",
    "\n",
    "(total_degree.join(in_degree, \"id\", how=\"left\")\n",
    " .join(out_degree, \"id\", how=\"left\")\n",
    " .fillna(0)\n",
    " .sort(\"inDegree\", ascending=False)\n",
    ".show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphframes.lib import AggregateMessages as AM \n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_paths(paths):\n",
    "    return F.collect_set(paths)\n",
    "\n",
    "\n",
    "collect_paths_udf = F.udf(collect_paths, ArrayType(StringType()))\n",
    "\n",
    "paths_type = ArrayType(\n",
    "    StructType([StructField(\"id\", StringType()), StructField(\"distance\",                                                    IntegerType())]))\n",
    "\n",
    "\n",
    "def flatten(ids):\n",
    "    flat_list = [item for sublist in ids for item in sublist]\n",
    "    return list(dict(sorted(flat_list, key=itemgetter(0))).items())\n",
    "\n",
    "\n",
    "flatten_udf = F.udf(flatten, paths_type)\n",
    "\n",
    "\n",
    "def new_paths(paths, id):\n",
    "    paths = [{\"id\": col1, \"distance\": col2 + 1} for col1,\n",
    "                            col2 in paths if col1 != id]\n",
    "    paths.append({\"id\": id, \"distance\": 1})\n",
    "    return paths\n",
    "\n",
    "\n",
    "new_paths_udf = F.udf(new_paths, paths_type)\n",
    "\n",
    "\n",
    "def merge_paths(ids, new_ids, id):\n",
    "    joined_ids = ids + (new_ids if new_ids else [])\n",
    "    merged_ids = [(col1, col2) for col1, col2 in joined_ids if col1 != id]\n",
    "    best_ids = dict(sorted(merged_ids, key=itemgetter(1), reverse=True))\n",
    "    return [{\"id\": col1, \"distance\": col2} for col1, col2 in best_ids.items()]\n",
    "\n",
    "\n",
    "merge_paths_udf = F.udf(merge_paths, paths_type)\n",
    "\n",
    "\n",
    "def calculate_closeness(ids):\n",
    "    nodes = len(ids)\n",
    "    total_distance = sum([col2 for col1, col2 in ids])\n",
    "    return 0 if total_distance == 0 else nodes * 1.0 / total_distance\n",
    "\n",
    "\n",
    "closeness_udf = F.udf(calculate_closeness, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices = g.vertices.withColumn(\"ids\", F.array())\n",
    "cached_vertices = AM.getCachedDataFrame(vertices)\n",
    "g2 = GraphFrame(cached_vertices, g.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id='Alice', ids=[]),\n",
       " Row(id='Bridget', ids=[]),\n",
       " Row(id='Charles', ids=[]),\n",
       " Row(id='Doug', ids=[]),\n",
       " Row(id='Mark', ids=[]),\n",
       " Row(id='Michael', ids=[]),\n",
       " Row(id='David', ids=[]),\n",
       " Row(id='Amy', ids=[]),\n",
       " Row(id='James', ids=[])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g2.vertices.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------------------------------------------------+------------------+\n",
      "|id     |ids                                                              |closeness         |\n",
      "+-------+-----------------------------------------------------------------+------------------+\n",
      "|Alice  |[[Charles, 1], [Mark, 1], [Bridget, 1], [Doug, 1], [Michael, 1]] |1.0               |\n",
      "|Doug   |[[Charles, 1], [Mark, 1], [Alice, 1], [Bridget, 1], [Michael, 1]]|1.0               |\n",
      "|David  |[[James, 1], [Amy, 1]]                                           |1.0               |\n",
      "|Bridget|[[Charles, 2], [Mark, 2], [Alice, 1], [Doug, 1], [Michael, 1]]   |0.7142857142857143|\n",
      "|Michael|[[Charles, 2], [Mark, 2], [Alice, 1], [Doug, 1], [Bridget, 1]]   |0.7142857142857143|\n",
      "|James  |[[Amy, 2], [David, 1]]                                           |0.6666666666666666|\n",
      "|Amy    |[[James, 2], [David, 1]]                                         |0.6666666666666666|\n",
      "|Mark   |[[Bridget, 2], [Charles, 2], [Michael, 2], [Doug, 1], [Alice, 1]]|0.625             |\n",
      "|Charles|[[Bridget, 2], [Mark, 2], [Michael, 2], [Doug, 1], [Alice, 1]]   |0.625             |\n",
      "+-------+-----------------------------------------------------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, g2.vertices.count()):\n",
    "    msg_dst = new_paths_udf(AM.src[\"ids\"], AM.src[\"id\"])\n",
    "    msg_src = new_paths_udf(AM.dst[\"ids\"], AM.dst[\"id\"])\n",
    "    agg = g2.aggregateMessages(F.collect_set(AM.msg).alias(\"agg\"),\n",
    "                               sendToSrc=msg_src, sendToDst=msg_dst)\n",
    "    res = agg.withColumn(\"newIds\", flatten_udf(\"agg\")).drop(\"agg\")\n",
    "    new_vertices = (g2.vertices.join(res, on=\"id\", how=\"left_outer\")\n",
    "                    .withColumn(\"mergedIds\", merge_paths_udf(\"ids\", \"newIds\",\n",
    "                    \"id\")).drop(\"ids\", \"newIds\")\n",
    "                    .withColumnRenamed(\"mergedIds\", \"ids\"))\n",
    "    cached_new_vertices = AM.getCachedDataFrame(new_vertices)\n",
    "    g2 = GraphFrame(cached_new_vertices, g2.edges)\n",
    "\n",
    "(g2.vertices\n",
    " .withColumn(\"closeness\", closeness_udf(\"ids\"))\n",
    " .sort(\"closeness\", ascending=False)\n",
    " .show(truncate=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
